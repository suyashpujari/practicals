{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e9627a4-d762-4530-be6b-338f02ad4050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADITYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADITYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ADITYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADITYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('stopwords') \n",
    "from nltk import sent_tokenize \n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac79b069-6c25-47af-9df1-39e19a49d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e948674-26ff-4c1f-be22-7648e09e1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. NLP techniques are used for tasks such as sentiment analysis, language translation, named entity recognition, and text summarization. This field has seen significant advancements in recent years, thanks to the availability of large-scale datasets and advances in machine learning algorithms.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c76597aa-86e8-4dd6-b51a-b41c28ca7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Document:\n",
      " ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'NLP', 'techniques', 'are', 'used', 'for', 'tasks', 'such', 'as', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'named', 'entity', 'recognition', ',', 'and', 'text', 'summarization', '.', 'This', 'field', 'has', 'seen', 'significant', 'advancements', 'in', 'recent', 'years', ',', 'thanks', 'to', 'the', 'availability', 'of', 'large-scale', 'datasets', 'and', 'advances', 'in', 'machine', 'learning', 'algorithms', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "print(\"Tokenized Document:\\n\", tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b65882d-aadb-4954-990c-007311e47bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'NLP', 'techniques', 'are', 'used', 'for', 'tasks', 'such', 'as', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'named', 'entity', 'recognition', ',', 'and', 'text', 'summarization', '.', 'This', 'field', 'has', 'seen', 'significant', 'advancements', 'in', 'recent', 'years', ',', 'thanks', 'to', 'the', 'availability', 'of', 'large-scale', 'datasets', 'and', 'advances', 'in', 'machine', 'learning', 'algorithms', '.']\n"
     ]
    }
   ],
   "source": [
    "#POS(parts of speech) tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7574f261-a653-41d0-8f8b-1f3e6bf4d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#stopwords\n",
    "sw_nltk = stopwords.words('english') \n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "154c55f2-230b-492f-a4b2-84e4ad413a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (after stop words removal):\n",
      " ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.', 'NLP', 'techniques', 'used', 'tasks', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'named', 'entity', 'recognition', ',', 'text', 'summarization', '.', 'field', 'seen', 'significant', 'advancements', 'recent', 'years', ',', 'thanks', 'availability', 'large-scale', 'datasets', 'advances', 'machine', 'learning', 'algorithms', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered Tokens (after stop words removal):\\n\", filtered_tokens, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "51baada1-99ed-48e9-9684-f72febfb31e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens:\n",
      " ['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'linguist', ',', 'comput', 'scienc', ',', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', ',', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data', '.', 'nlp', 'techniqu', 'use', 'task', 'sentiment', 'analysi', ',', 'languag', 'translat', ',', 'name', 'entiti', 'recognit', ',', 'text', 'summar', '.', 'field', 'seen', 'signific', 'advanc', 'recent', 'year', ',', 'thank', 'avail', 'large-scal', 'dataset', 'advanc', 'machin', 'learn', 'algorithm', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Perform Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\\n\", stemmed_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e29296ec-42b8-4bd3-8422-f1f82a510971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:\n",
      " ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', ',', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', '.', 'NLP', 'technique', 'used', 'task', 'sentiment', 'analysis', ',', 'language', 'translation', ',', 'named', 'entity', 'recognition', ',', 'text', 'summarization', '.', 'field', 'seen', 'significant', 'advancement', 'recent', 'year', ',', 'thanks', 'availability', 'large-scale', 'datasets', 'advance', 'machine', 'learning', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "# Perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\\n\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7859986c-c0c9-41b4-928e-b3c25f261e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1\n",
      "  advancements : 0.11704114719613057\n",
      "  advances : 0.11704114719613057\n",
      "  algorithms : 0.11704114719613057\n",
      "  amounts : 0.11704114719613057\n",
      "  analysis : 0.11704114719613057\n",
      "  analyze : 0.11704114719613057\n",
      "  artificial : 0.11704114719613057\n",
      "  availability : 0.11704114719613057\n",
      "  computer : 0.11704114719613057\n",
      "  computers : 0.23408229439226114\n",
      "  concerned : 0.11704114719613057\n",
      "  data : 0.11704114719613057\n",
      "  datasets : 0.11704114719613057\n",
      "  entity : 0.11704114719613057\n",
      "  field : 0.11704114719613057\n",
      "  human : 0.11704114719613057\n",
      "  intelligence : 0.11704114719613057\n",
      "  interactions : 0.11704114719613057\n",
      "  language : 0.4681645887845223\n",
      "  large : 0.23408229439226114\n",
      "  learning : 0.11704114719613057\n",
      "  linguistics : 0.11704114719613057\n",
      "  machine : 0.11704114719613057\n",
      "  named : 0.11704114719613057\n",
      "  natural : 0.23408229439226114\n",
      "  nlp : 0.23408229439226114\n",
      "  particular : 0.11704114719613057\n",
      "  process : 0.11704114719613057\n",
      "  processing : 0.11704114719613057\n",
      "  program : 0.11704114719613057\n",
      "  recent : 0.11704114719613057\n",
      "  recognition : 0.11704114719613057\n",
      "  scale : 0.11704114719613057\n",
      "  science : 0.11704114719613057\n",
      "  seen : 0.11704114719613057\n",
      "  sentiment : 0.11704114719613057\n",
      "  significant : 0.11704114719613057\n",
      "  subfield : 0.11704114719613057\n",
      "  summarization : 0.11704114719613057\n",
      "  tasks : 0.11704114719613057\n",
      "  techniques : 0.11704114719613057\n",
      "  text : 0.11704114719613057\n",
      "  thanks : 0.11704114719613057\n",
      "  translation : 0.11704114719613057\n",
      "  used : 0.11704114719613057\n",
      "  years : 0.11704114719613057\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(document)\n",
    "filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "tokenized_document = \" \".join(filtered_tokens)\n",
    "\n",
    "# Create a tf-idf vectorizer and fit it to the tokenized document\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([tokenized_document])\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the document-term matrix\n",
    "print(\"Document 1\")\n",
    "for j, feature in enumerate(feature_names):\n",
    " if tfidf_matrix[0,j] > 0:\n",
    "   print(\" \", feature, \":\", tfidf_matrix[0,j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
